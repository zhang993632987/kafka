# 6.4 在可靠的系统中使用消费者

**只有已经被提交到Kafka的数据，也就是已经被写入所有同步副本的数据，对消费者是可用的**。这保证了消费者读取到的数据是一致的。<mark style="color:orange;">**消费者唯一要做的是跟踪哪些消息是已经读取过的，哪些消息是还未读取的，这是消费者在读取消息时不丢失消息的关键。**</mark>

## 6.4.1 消费者的可靠性配置

为了保证消费者行为的可靠性，需要注意以下4个非常重要的配置参数：

* **group.id**：如果两个消费者具有相同的群组ID，并订阅了同一个主题，那么每个消费者将分到主题分区的一个子集，也就是说它们只能读取到所有消息的一个子集（但整个群组可以读取到主题所有的消息）。如果你希望一个消费者可以读取主题所有的消息，那么就需要为它设置唯一的 group.id。
* **auto.offset.reset**：这个参数指定了当没有偏移量（比如在消费者首次启动时）或请求的偏移量在broker上不存在时消费者该作何处理。这个参数有两个值，一个是 **earliest**，如果配置了这个值，那么消费者将从分区的开始位置读取数据，这会导致消费者读取大量的重复数据，但可以保证最少的数据丢失。另一个值是 **latest**，如果配置了这个值，那么消费者将从分区的末尾位置读取数据，这样可以减少重复处理消息，但很有可能会错过一些消息。
* **enable.auto.commit**：你可以决定让消费者自动提交偏移量，也可以在代码里手动提交偏移量。自动提交的一个最大好处是可以少操心一些事情。如果是在消费者的消息轮询里处理数据，那么自动提交可以确保不会意外提交未处理的偏移量。自动提交的主要缺点是我们无法控制应用程序可能重复处理的消息的数量，比如消费者在还没有触发自动提交之前处理了一些消息，然后被关闭。如果应用程序的处理逻辑比较复杂（比如把消息交给另外一个后台线程去处理），那么就只能使用手动提交了，因为自动提交机制有可能会在还没有处理完消息时就提交偏移量。
* **auto.commit.interval.ms**：如果选择使用自动提交，那么可以通过这个参数来控制提交的频率，默认每5秒提交一次。一般来说，**频繁提交会增加额外的开销，但也会降低重复处理消息的概率**。

## 6.4.2 手动提交偏移量

如果想要更大的灵活性，选择了手动提交，那么就需要考虑正确性和性能方面的问题：

1.  **总是在处理完消息后提交偏移量**

    在轮询过程中提交偏移量有一个缺点，就是有可能会意外提交已读取但未处理的消息的偏移量。一定要在处理完消息后再提交偏移量，这点很关键——提交已读取但未处理的消息的偏移量会导致消费者错过消息。
2.  **提交频率是性能和重复消息数量之间的权衡**

    提交频率需要在性能需求和重复消息量之间取得平衡。处理一条消息就提交一次偏移量的方式只适用于吞吐量非常低的主题。
3.  **再均衡**

    在设计应用程序时，需要考虑到消费者会发生再均衡并需要处理好它们。
4.  **消费者可能需要重试**

    有时候，在调用了轮询方法之后，有些消息需要稍后再处理。需要注意的是，消费者提交偏移量并不是对单条消息的“确认”，这与传统的发布和订阅消息系统不一样。也就是说，如果记录 #30处理失败，但记录 #31处理成功，那么就不应该提交记录 #31的偏移量——如果提交了，就表示 #31以内的记录都已处理完毕，包括记录 #30在内，但这可能不是我们想要的结果。不过，可以采用以下两种模式来解决这个问题：

    * 第一种模式，在遇到可重试错误时，提交最后一条处理成功的消息的偏移量，然后把还未处理好的消息保存到缓冲区（这样下一个轮询就不会把它们覆盖掉），并调用消费者的 **pause()** 方法，确保其他的轮询不会返回数据，之后继续处理缓冲区里的消息。
    * 第二种模式，在遇到可重试错误时，把消息写到另一个**重试主题**，并继续处理其他消息。另一个消费者群组负责处理重试主题中的消息，或者让一个消费者同时订阅主主题和重试主题。这种模式有点儿像其他消息系统中的死信队列。
5.  **消费者可能需要维护状态**

    在一些应用程序中，需要维护多个轮询之间的状态。如果想计算移动平均数，就需要在每次轮询之后更新结果。如果应用程序重启，则不仅需要从上一个偏移量位置开始处理消息，还需要恢复之前保存的移动平均数。**一种办法是在提交偏移量的同时把算好的移动平均数写到一个“结果”主题中。**当一个线程重新启动时，它就可以获取到之前算好的移动平均数，并从上一次提交的偏移量位置开始读取数据。

    一般来说，这是一个比较复杂的问题，建议尝试使用其他框架，比如Kafka Streams或Flink，它们为聚合、连接、时间窗和其他复杂的分析操作提供了高级的DSL API。
